{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5504904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf1d0e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_tags(string):\n",
    "    removelist = \"\"  # Add any characters you'd like to keep\n",
    "    # Remove HTML tags\n",
    "    result = re.sub(r'<[^>]+>', '', string)\n",
    "    # Remove URLs\n",
    "    result = re.sub(r'https?://\\S+', '', result)\n",
    "    # Remove non-alphanumeric characters (except for those in the removelist)\n",
    "    result = re.sub(r'[^a-zA-Z0-9' + removelist + r'\\s]', ' ', result)\n",
    "    # Convert to lowercase\n",
    "    result = result.lower()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7ba5034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production  the filming tec...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there s a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei s  love in the time of money  is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  one of the other reviewers has mentioned that ...          1\n",
       "1  a wonderful little production  the filming tec...          1\n",
       "2  i thought this was a wonderful way to spend ti...          1\n",
       "3  basically there s a family where a little boy ...          0\n",
       "4  petter mattei s  love in the time of money  is...          1"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "df['review'] = df['review'].apply(remove_tags)\n",
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1580020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word = set(stopwords.words('english'))\n",
    " \n",
    "df['review'] = df['review'].apply(lambda x:' '.join([word for word in x.split() if word not in (stop_word)]))\n",
    " \n",
    "# Melakukan split dataset\n",
    "review = df['review'].values\n",
    "sentiment = df['sentiment'].values\n",
    " \n",
    "review_train, review_test, sentiment_train, sentiment_test = train_test_split(review, sentiment, test_size=0.1, shuffle=False)\n",
    " \n",
    "# Membuat tokenisasi\n",
    "filt = '!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ' # Filter untuk menghilangkan symbols\n",
    "\n",
    "vocab_size = 10000\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\", filters=filt)\n",
    " \n",
    "tokenizer.fit_on_texts(review_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "32e1caf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 119.5824\n",
      "90% review length: 236\n",
      "95% review length: 309\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Hitung panjang setiap review di data latih\n",
    "review_lengths = [len(x) for x in df['review'].apply(lambda x: x.split())]\n",
    "\n",
    "# Cek persentil 90 atau 95\n",
    "p90 = int(np.percentile(review_lengths, 90))\n",
    "p95 = int(np.percentile(review_lengths, 95))\n",
    "\n",
    "print(f\"Mean: {np.mean(review_lengths)}\")\n",
    "print(f\"90% review length: {p90}\")\n",
    "print(f\"95% review length: {p95}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8baccb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Menyimpan word_index kedalam sebuah file json\n",
    "filtered_word_index = {word: index for word, index in tokenizer.word_index.items() if index < vocab_size}\n",
    " \n",
    "with open('word_index.json', 'w') as fp:\n",
    "    json.dump(filtered_word_index, fp)\n",
    " \n",
    "# Membuat sequences dan melakukan padding\n",
    "train_sekuens = tokenizer.texts_to_sequences(review_train)\n",
    "test_sekuens = tokenizer.texts_to_sequences(review_test)\n",
    " \n",
    "train_padded = pad_sequences(train_sekuens,\n",
    "                             maxlen=p90,\n",
    "                             padding='pre', truncating='pre')\n",
    "test_padded = pad_sequences(test_sekuens,\n",
    "                            maxlen=p90,\n",
    "                            padding='pre', truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d51361f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "08cfae29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:100: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 10ms/step - accuracy: 0.7873 - loss: 0.4294 - val_accuracy: 0.8852 - val_loss: 0.2819\n",
      "Epoch 2/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.8938 - loss: 0.2638 - val_accuracy: 0.8842 - val_loss: 0.2751\n",
      "Epoch 3/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - accuracy: 0.9083 - loss: 0.2350 - val_accuracy: 0.8634 - val_loss: 0.3218\n",
      "Epoch 4/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 9ms/step - accuracy: 0.9134 - loss: 0.2203 - val_accuracy: 0.8958 - val_loss: 0.2593\n",
      "Epoch 5/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 10ms/step - accuracy: 0.9205 - loss: 0.2045 - val_accuracy: 0.8482 - val_loss: 0.3977\n",
      "Epoch 6/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 11ms/step - accuracy: 0.9247 - loss: 0.1945 - val_accuracy: 0.8676 - val_loss: 0.3407\n",
      "Epoch 7/30\n",
      "\u001b[1m1407/1407\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 10ms/step - accuracy: 0.9286 - loss: 0.1860 - val_accuracy: 0.8926 - val_loss: 0.2819\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "# Membuat model\n",
    "model = tf.keras.Sequential([\n",
    "    Embedding(vocab_size, 64, input_length=p90),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu'),\n",
    "    \n",
    "    Dense(1, activation='sigmoid')\n",
    "\n",
    "])\n",
    " \n",
    "# Compile model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "# Train model\n",
    "num_epochs = 30\n",
    "history = model.fit(train_padded, sentiment_train,\n",
    "                    epochs=num_epochs,\n",
    "                    validation_data=(test_padded, sentiment_test),\n",
    "                    verbose=1,\n",
    "                    callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0006ac7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8958 - loss: 0.2593\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2593337893486023, 0.895799994468689]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_padded, sentiment_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b72d6f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tf_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tf_model\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(model, 'tf_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f2bc0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text):\n",
    "    sequence = tokenizer.texts_to_sequences([text])\n",
    "    padded = pad_sequences(sequence, maxlen=p90, padding='post', truncating='post')\n",
    "    prediction = model.predict(padded)\n",
    "    return f\"positive (confidence :{prediction[0][0]:.4f})\" if prediction[0][0] >= 0.5 else f\"negative (confidence :{1 - prediction[0][0]:.4f})\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc1e487f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'positive (confidence :0.9680)'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_sentiment(\"\"\"let's get this out of the way first: I REALLY liked Monster! But, I wouldn't watch it again...\n",
    "\n",
    "Monster is a down-to-Earth psychological crime drama. At first it might appear to have supernatural elements, but I won't spoil it for you. After watching the entire thing, I would say Monster would've been great as a live-action TV series, which couldn't be said for any other anime I watched so far (and I watched quite a lot). It has everything a great series needs, a cast of characters that the audience can care about, action-packed scenes to liven up the pretty in-depth crime drama, great character development, a good story full of twists and turn that will keep you coming back for more and an astonishing and realistic art-style to wrap it all together. During it's 74 episodes it brings up many hard questions about the human psyche, morality and human connections, relationships. How far are we willing to go to accomplish our goals? How much of your humanity are you willing to trade in for them? What is \"humanity\" anyway? Some of the episodes are frighteningly realistic in describing the human condition and it doesn't back down from touching really hard social and historical taboos either. No, it grabs you by the hair and slams your face in them saying \"Look! That's what you are!\". I can safely say Monster was one of the most unique and thought-provoking experiences I've ever had.\n",
    "\n",
    "But, (and yes, here comes the \"but\") Monster is anything but perfect. While it's action-packed and suspenseful story would stand great on it's own, it's sadly spread too thin and too long. The story needlessly drags on for 74 episodes and the ending feels more like a coup de grace than closure. The cast of main characters is huge, and while they are really well made and fleshed out, we are continuously introduced to a slew of new side-characters that have barely any relevance to the main story (if at all). The writers regularly go off on tangents just to demonstrate a small plot point or tidbits of (mostly irrelevant) character backstory, bringing in and taking out characters on a whim after they \"served their purpose\". In the end, they were seemingly just struggling to give enough individual screen-time to their monstrously bloated cast (pun intended), and it only makes the audience lose interest in them and lose count on who is who why they are even there.\n",
    "\n",
    "The story is mostly delivered in (sometimes painfully dragged out) exposition. The story lurches forward in needlessly detailed investigation sequences flooded with meaningless trivia and extra character backstory that have little to no bearing on the main story itself. The story regularly branches off into dead-ends and meaningless side-plots that fill entire episodes yet don't bring anything new or interesting to the table. The sheer amount of dialogue and narration in Monster would fill entire volumes of books. Even the exposition itself is riddled by double-takes, needlessly repeated \"remembering\" segments and a ton of redundant, rephrased information. You can seriously skip entire dozens of episodes and still understand everything since the characters and the exposition keep repeating themselves over and over. The whole series could've been distilled down to a neat 30-35 episodes without losing any of the story.\n",
    "\n",
    "My third (entirely personal) beef is with the setting. Unlike most anime, the entire story of Monster takes place in 80's and 90's Europe (mostly Germany and the former Czechoslovakia). See, I was born and still living in Europe, I lived in the time and place the show takes place. Monster being a work of fiction, I chalked up most of the factual, cultural and historical errors to \"writer's freedom\" and \"suspension of disbelief\" and such. While the creators of the anime obviously did their homework and got most of the big things right, there were some things that bugged me more than they should've. Little, insignificant things that most people from other parts of the world would miss, were just screaming at me from the screen. Getting used to the obviously Japanese mannerisms, phrases and behaviors forced upon the allegedly European characters is one thing. But small details like choice of words, type of foods/drinks, fashion, architecture or even music in some places were just flat our wrong and felt so out of place that it shoved me right out of the immersion. I know it sounds lame, but since the creators obviously tried to recreate the setting realistically, I just couldn't help it.\n",
    "\n",
    "I know I spent most of this review pandering on what's wrong with Monster, but the truth is, I really liked it, and I stand by my score of 8/10. It's really worth watching for everyone who desires something other than the run-of-mill anime, something unique. If you can overlook the droll exposition and sometimes aimlessly branching and dragged out storytelling, you'll find a really suspenseful and interesting story of crime and punishment, dark secrets, interesting characters, huge plot twists, thrilling psychological expeditions into the human mind and soul and much more.\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8dc0ee76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.build(input_shape=(None, 236))\n",
    "model.save('model_fixed.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
